{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "829f0ff9",
   "metadata": {},
   "source": [
    "# Stat 5241 Statistical Machine Learning Final Project\n",
    "## Author: Gexin Chen     Uni: gc2936"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23489e8",
   "metadata": {},
   "source": [
    "# 1. BackGround"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45effaf6",
   "metadata": {},
   "source": [
    "<font size=\"3\">The MNIST database of handwritten digits is one of the most commonly used dataset for training\n",
    "various image processing systems and machine learning algorithms. MNIST has a training set of\n",
    "60,000 examples, and a test set of 10,000 examples. It is a good database for people who want to\n",
    "try learning techniques and pattern recognition methods on real-world data while spending minimal\n",
    "efforts on preprocessing and formatting.\n",
    "MNIST is a subset of a larger set available from NIST. The digits have been size-normalized\n",
    "and centered in a fixed-size image. The original black and white (bilevel) images from NIST were\n",
    "size normalized. The resulting images contain grey levels as a result of the anti-aliasing technique\n",
    "used by the normalization algorithm. The images were centered in a 28 ×28 image by computing\n",
    "the center of mass of the pixels, and translating the image so as to position this point at the center\n",
    "of the 28 ×28 field.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa62567b",
   "metadata": {},
   "source": [
    "# 2. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb93d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Packages\n",
    "import time\n",
    "import pandas as pd\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a55333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gexin/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/Users/gexin/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/Users/gexin/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/Users/gexin/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "# Data download and preprocessing\n",
    "\n",
    "DOWNLOAD_MNIST = False # If already download , set as False\n",
    "train_data = MNIST(\n",
    " root='../data/mnist/',\n",
    " train=True , # this is training data\n",
    " #transform=torchvision.transforms.ToTensor (),\n",
    " download=DOWNLOAD_MNIST ,\n",
    " )\n",
    "\n",
    "test_data = MNIST(root='../data/ mnist/', train=False, download=DOWNLOAD_MNIST)\n",
    "\n",
    " # change the features to numpy\n",
    "X_train = train_data.train_data.numpy()\n",
    "X_test = test_data.test_data.numpy ()\n",
    "\n",
    " # change the labels to numpy\n",
    "Y_train = train_data.train_labels.numpy()\n",
    "Y_test = test_data.test_labels.numpy ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9056d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_X_train = X_train.reshape(-1,28,28,1)\n",
    "my_X_test = X_test.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39da7d6",
   "metadata": {},
   "source": [
    "## (a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f461d7be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot The Feature Parameters\n",
    "imshow(X_train[7]);\n",
    "\n",
    "# Return the Output \n",
    "print(Y_train[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac93d52d",
   "metadata": {},
   "source": [
    "From above image, we can see the number is '3' which matches the label in Y_train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799d12a9",
   "metadata": {},
   "source": [
    "## (b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ac1239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10000, 28, 28), (60000, 28, 28)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Dimensions\n",
    "[shape(X_test),shape(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a353872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reshape\n",
    "X_train = X_train.reshape(-1,28*28)\n",
    "X_test = X_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d92a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization\n",
    "X_train = preprocessing.normalize(X_train)\n",
    "X_test = preprocessing.normalize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f7939",
   "metadata": {},
   "source": [
    "## (c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5248511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "onehot = preprocessing.OneHotEncoder(sparse = False)\n",
    "onehot.fit(Y_train.reshape(-1,1))\n",
    "Y_train_oh = onehot.transform(Y_train.reshape(-1,1))\n",
    "Y_test_oh = onehot.transform(Y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd39fd0",
   "metadata": {},
   "source": [
    "One hot embedding transfer every labels to 1*N matrix containing only 0 and 1. In this way, each label can be treated equally by machine learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469fa4cb",
   "metadata": {},
   "source": [
    "# 3. Before Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d1d8f",
   "metadata": {},
   "source": [
    "## (a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "005d0c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN training\n",
    "KNN = KNeighborsClassifier(n_neighbors=3).fit(X_train, Y_train_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5be13d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02729999999999999"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - KNN.score(X_test, Y_test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7534b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost Training\n",
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3)).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adaef6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1965"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - ada.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92fdb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with Gaussian Kernel\n",
    "svm = SVC().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0a09574",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018900000000000028"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - svm.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa260f3",
   "metadata": {},
   "source": [
    "From above, we can see that the test errors of all three algorithms are different from the given result. It can be caused by multiple reasons:  \n",
    "1. The data we used above may be different from the data authors used;\n",
    "2. The preprocessing procedures are different;\n",
    "3. The hyperparameters for the three algorithms may be different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ec314",
   "metadata": {},
   "source": [
    "## (b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f7d086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables Assign\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47bd4bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 13:53:36.483944: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "mymodel = Sequential()\n",
    "mymodel.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1),padding='same'))\n",
    "mymodel.add(LeakyReLU(alpha=0.1))\n",
    "mymodel.add(MaxPooling2D((2, 2),padding='same'))\n",
    "mymodel.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "mymodel.add(LeakyReLU(alpha=0.1))\n",
    "mymodel.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "mymodel.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "mymodel.add(LeakyReLU(alpha=0.1))                  \n",
    "mymodel.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "mymodel.add(Flatten())\n",
    "mymodel.add(Dense(128, activation='linear'))\n",
    "mymodel.add(LeakyReLU(alpha=0.1))                  \n",
    "mymodel.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "885a61cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "938/938 [==============================] - 78s 82ms/step - loss: 0.4488 - accuracy: 0.9473\n",
      "Epoch 2/30\n",
      "938/938 [==============================] - 81s 87ms/step - loss: 0.0522 - accuracy: 0.9840\n",
      "Epoch 3/30\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.0379 - accuracy: 0.9881\n",
      "Epoch 4/30\n",
      "938/938 [==============================] - 73s 77ms/step - loss: 0.0358 - accuracy: 0.9890\n",
      "Epoch 5/30\n",
      "938/938 [==============================] - 74s 79ms/step - loss: 0.0298 - accuracy: 0.9910\n",
      "Epoch 6/30\n",
      "938/938 [==============================] - 69s 74ms/step - loss: 0.0290 - accuracy: 0.9908\n",
      "Epoch 7/30\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.0311 - accuracy: 0.9908\n",
      "Epoch 8/30\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.0311 - accuracy: 0.9913\n",
      "Epoch 9/30\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0252 - accuracy: 0.9924\n",
      "Epoch 10/30\n",
      "938/938 [==============================] - 70s 75ms/step - loss: 0.0235 - accuracy: 0.9933\n",
      "Epoch 11/30\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.0263 - accuracy: 0.9927\n",
      "Epoch 12/30\n",
      "938/938 [==============================] - 73s 78ms/step - loss: 0.0285 - accuracy: 0.9927\n",
      "Epoch 13/30\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.0145 - accuracy: 0.9959\n",
      "Epoch 14/30\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.0240 - accuracy: 0.9941\n",
      "Epoch 15/30\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.0199 - accuracy: 0.9950\n",
      "Epoch 16/30\n",
      "938/938 [==============================] - 72s 77ms/step - loss: 0.0241 - accuracy: 0.9944\n",
      "Epoch 17/30\n",
      "938/938 [==============================] - 79s 84ms/step - loss: 0.0258 - accuracy: 0.9943\n",
      "Epoch 18/30\n",
      "938/938 [==============================] - 81s 86ms/step - loss: 0.0184 - accuracy: 0.9956\n",
      "Epoch 19/30\n",
      "938/938 [==============================] - 96s 102ms/step - loss: 0.0249 - accuracy: 0.9951\n",
      "Epoch 20/30\n",
      "938/938 [==============================] - 77s 82ms/step - loss: 0.0188 - accuracy: 0.9960\n",
      "Epoch 21/30\n",
      "938/938 [==============================] - 69s 73ms/step - loss: 0.0242 - accuracy: 0.9955\n",
      "Epoch 22/30\n",
      "938/938 [==============================] - 75s 80ms/step - loss: 0.0252 - accuracy: 0.9951\n",
      "Epoch 23/30\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.0177 - accuracy: 0.9966\n",
      "Epoch 24/30\n",
      "938/938 [==============================] - 71s 75ms/step - loss: 0.0242 - accuracy: 0.9956\n",
      "Epoch 25/30\n",
      "938/938 [==============================] - 79s 85ms/step - loss: 0.0230 - accuracy: 0.9962\n",
      "Epoch 26/30\n",
      "938/938 [==============================] - 71s 76ms/step - loss: 0.0259 - accuracy: 0.9958\n",
      "Epoch 27/30\n",
      "938/938 [==============================] - 67s 72ms/step - loss: 0.0292 - accuracy: 0.9958\n",
      "Epoch 28/30\n",
      "938/938 [==============================] - 66s 70ms/step - loss: 0.0312 - accuracy: 0.9960\n",
      "Epoch 29/30\n",
      "938/938 [==============================] - 92s 98ms/step - loss: 0.0234 - accuracy: 0.9970\n",
      "Epoch 30/30\n",
      "938/938 [==============================] - 77s 82ms/step - loss: 0.0371 - accuracy: 0.9957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff89dcf9e50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "mymodel.fit(my_X_train, Y_train_oh, batch_size=batch_size,epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e19c1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 11ms/step - loss: 0.1786 - accuracy: 0.9861\n"
     ]
    }
   ],
   "source": [
    "test_error = mymodel.evaluate(my_X_test, Y_test_oh)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3913280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test error of my favorite classifier is: 0.01389998197555542\n"
     ]
    }
   ],
   "source": [
    "print('The test error of my favorite classifier is:', 1 - test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1160e9b8",
   "metadata": {},
   "source": [
    "It can be observed that the test error is lower than that of all three algorithms from part (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7d67d",
   "metadata": {},
   "source": [
    "# 4. Deep Leaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19a232e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b6158fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Type Convert\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Read Data\n",
    "train_data = MNIST(root='../data/mnist/', train=True, download=False, transform = transform)\n",
    "\n",
    "test_data = MNIST(root='../data/ mnist/', train=False, download=False, transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab1fce3",
   "metadata": {},
   "source": [
    "## 3. (a) &  (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b2d9efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Fully Connected Neural Network\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ANN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(input_size, 100)\n",
    "        self.layer2 = nn.Linear(100, num_classes)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.sigmoid(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc1bb7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    running_loss, correct = 0, 0\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    running_loss = running_loss / size\n",
    "    correct /= size\n",
    "    print('Training loss: {}'.format(running_loss))\n",
    "    print('Training accuracy: {}'.format(correct))\n",
    "    return running_loss, correct\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print('Testing loss: {}'.format(test_loss))\n",
    "    print('Testing accuracy: {}'.format(correct))\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505b40fb",
   "metadata": {},
   "source": [
    "### Seed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20141d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.1 \n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "model1 = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model1.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "069e8ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 0.6312321174780527\n",
      "Training accuracy: 0.8388166666666667\n",
      "Testing loss: 0.32605285197496414\n",
      "Testing accuracy: 0.9114\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training loss: 0.30747619849443436\n",
      "Training accuracy: 0.9101333333333333\n",
      "Testing loss: 0.2652289485855467\n",
      "Testing accuracy: 0.9222\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Training loss: 0.25936088887055714\n",
      "Training accuracy: 0.9242333333333334\n",
      "Testing loss: 0.2467449258087547\n",
      "Testing accuracy: 0.9275\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Training loss: 0.22716621174017587\n",
      "Training accuracy: 0.9343833333333333\n",
      "Testing loss: 0.2099801887325041\n",
      "Testing accuracy: 0.9393\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Training loss: 0.20135708603461583\n",
      "Training accuracy: 0.94155\n",
      "Testing loss: 0.20063233762315125\n",
      "Testing accuracy: 0.9412\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Training loss: 0.1818713160375754\n",
      "Training accuracy: 0.9471166666666667\n",
      "Testing loss: 0.17156985816160206\n",
      "Testing accuracy: 0.9494\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Training loss: 0.16503312250375748\n",
      "Training accuracy: 0.9521333333333334\n",
      "Testing loss: 0.16374759309610742\n",
      "Testing accuracy: 0.9513\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Training loss: 0.15132120109796524\n",
      "Training accuracy: 0.9564333333333334\n",
      "Testing loss: 0.161040210420159\n",
      "Testing accuracy: 0.9523\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Training loss: 0.13996072180469832\n",
      "Training accuracy: 0.9595166666666667\n",
      "Testing loss: 0.14541404543646202\n",
      "Testing accuracy: 0.9571\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Training loss: 0.13020200232863427\n",
      "Training accuracy: 0.9628166666666667\n",
      "Testing loss: 0.1323606773356723\n",
      "Testing accuracy: 0.9606\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Training loss: 0.12150704482595126\n",
      "Training accuracy: 0.9650166666666666\n",
      "Testing loss: 0.13427240313712957\n",
      "Testing accuracy: 0.9612\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Training loss: 0.11398574720819792\n",
      "Training accuracy: 0.9678166666666667\n",
      "Testing loss: 0.12639727727005814\n",
      "Testing accuracy: 0.9622\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Training loss: 0.10681332801183065\n",
      "Training accuracy: 0.9695666666666667\n",
      "Testing loss: 0.11848506357544547\n",
      "Testing accuracy: 0.9647\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Training loss: 0.10189377796550592\n",
      "Training accuracy: 0.9715\n",
      "Testing loss: 0.11363248516669035\n",
      "Testing accuracy: 0.9664\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Training loss: 0.09621494003931681\n",
      "Training accuracy: 0.97295\n",
      "Testing loss: 0.10666891451996223\n",
      "Testing accuracy: 0.9671\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Training loss: 0.09114216537276904\n",
      "Training accuracy: 0.9746166666666667\n",
      "Testing loss: 0.1047551214552609\n",
      "Testing accuracy: 0.9682\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Training loss: 0.0868209639877081\n",
      "Training accuracy: 0.9759\n",
      "Testing loss: 0.10329806789233806\n",
      "Testing accuracy: 0.9682\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Training loss: 0.08265571415225664\n",
      "Training accuracy: 0.9773166666666666\n",
      "Testing loss: 0.10220716120141327\n",
      "Testing accuracy: 0.9684\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Training loss: 0.07906140153557062\n",
      "Training accuracy: 0.9785333333333334\n",
      "Testing loss: 0.09585777705143782\n",
      "Testing accuracy: 0.9695\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Training loss: 0.07539224013586839\n",
      "Training accuracy: 0.9797833333333333\n",
      "Testing loss: 0.09567331377962593\n",
      "Testing accuracy: 0.9706\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Training loss: 0.07229305909276008\n",
      "Training accuracy: 0.9802333333333333\n",
      "Testing loss: 0.09714907073196333\n",
      "Testing accuracy: 0.97\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Training loss: 0.06903246783216795\n",
      "Training accuracy: 0.98145\n",
      "Testing loss: 0.0997185023726931\n",
      "Testing accuracy: 0.9709\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Training loss: 0.06638706729362408\n",
      "Training accuracy: 0.98225\n",
      "Testing loss: 0.09130564043713603\n",
      "Testing accuracy: 0.9715\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Training loss: 0.0635785830527544\n",
      "Training accuracy: 0.9829333333333333\n",
      "Testing loss: 0.09025384842234242\n",
      "Testing accuracy: 0.9732\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Training loss: 0.06127839450240135\n",
      "Training accuracy: 0.9836166666666667\n",
      "Testing loss: 0.08533021614287689\n",
      "Testing accuracy: 0.9742\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Training loss: 0.05905410729100307\n",
      "Training accuracy: 0.9846333333333334\n",
      "Testing loss: 0.08461978217100452\n",
      "Testing accuracy: 0.9739\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Training loss: 0.056904677504797774\n",
      "Training accuracy: 0.9852666666666666\n",
      "Testing loss: 0.08305285317574147\n",
      "Testing accuracy: 0.9738\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Training loss: 0.054453953233857946\n",
      "Training accuracy: 0.9860333333333333\n",
      "Testing loss: 0.08625281558840707\n",
      "Testing accuracy: 0.9728\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Training loss: 0.05274626832405726\n",
      "Training accuracy: 0.9863333333333333\n",
      "Testing loss: 0.08491130966906714\n",
      "Testing accuracy: 0.9743\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Training loss: 0.050896600427230196\n",
      "Training accuracy: 0.9871666666666666\n",
      "Testing loss: 0.0833016288211676\n",
      "Testing accuracy: 0.9753\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Training loss: 0.04903690692037344\n",
      "Training accuracy: 0.9878166666666667\n",
      "Testing loss: 0.08196763690727152\n",
      "Testing accuracy: 0.9743\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Training loss: 0.04759960208584865\n",
      "Training accuracy: 0.9881166666666666\n",
      "Testing loss: 0.08144234039577519\n",
      "Testing accuracy: 0.9736\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Training loss: 0.04606730470061302\n",
      "Training accuracy: 0.9883833333333333\n",
      "Testing loss: 0.07986198049216608\n",
      "Testing accuracy: 0.975\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Training loss: 0.04455727094064156\n",
      "Training accuracy: 0.989\n",
      "Testing loss: 0.08772501382371707\n",
      "Testing accuracy: 0.9737\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Training loss: 0.04286884162972371\n",
      "Training accuracy: 0.98965\n",
      "Testing loss: 0.08790081407234168\n",
      "Testing accuracy: 0.9737\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Training loss: 0.0414949409176906\n",
      "Training accuracy: 0.9901833333333333\n",
      "Testing loss: 0.07857880550347696\n",
      "Testing accuracy: 0.9757\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Training loss: 0.04023696773697932\n",
      "Training accuracy: 0.9905833333333334\n",
      "Testing loss: 0.0784410725816325\n",
      "Testing accuracy: 0.9758\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Training loss: 0.03915539190073808\n",
      "Training accuracy: 0.9908333333333333\n",
      "Testing loss: 0.07386640872758855\n",
      "Testing accuracy: 0.9776\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Training loss: 0.03779750686635574\n",
      "Training accuracy: 0.9914\n",
      "Testing loss: 0.07584405345189724\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Training loss: 0.03672922372023264\n",
      "Training accuracy: 0.99175\n",
      "Testing loss: 0.074365292785535\n",
      "Testing accuracy: 0.9763\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Training loss: 0.0358541913792491\n",
      "Training accuracy: 0.9916833333333334\n",
      "Testing loss: 0.07623604670238153\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Training loss: 0.03468198499704401\n",
      "Training accuracy: 0.9919333333333333\n",
      "Testing loss: 0.07358820981399458\n",
      "Testing accuracy: 0.9772\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Training loss: 0.033600581060349945\n",
      "Training accuracy: 0.99245\n",
      "Testing loss: 0.07306452501553354\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Training loss: 0.03276625404109557\n",
      "Training accuracy: 0.9927333333333334\n",
      "Testing loss: 0.07296618964422233\n",
      "Testing accuracy: 0.9766\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Training loss: 0.03185164739266038\n",
      "Training accuracy: 0.9931333333333333\n",
      "Testing loss: 0.0759282445351172\n",
      "Testing accuracy: 0.9755\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Training loss: 0.030896728436648847\n",
      "Training accuracy: 0.9934\n",
      "Testing loss: 0.07313415110941715\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Training loss: 0.029929666263858477\n",
      "Training accuracy: 0.9937666666666667\n",
      "Testing loss: 0.07394555885237968\n",
      "Testing accuracy: 0.9772\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Training loss: 0.02937405418281754\n",
      "Training accuracy: 0.9939833333333333\n",
      "Testing loss: 0.07374226104669795\n",
      "Testing accuracy: 0.9769\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Training loss: 0.02826606729204456\n",
      "Training accuracy: 0.9942833333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 0.07407478763941366\n",
      "Testing accuracy: 0.9773\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Training loss: 0.02773880961301426\n",
      "Training accuracy: 0.99435\n",
      "Testing loss: 0.07299135281010323\n",
      "Testing accuracy: 0.977\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Training loss: 0.026979105322311323\n",
      "Training accuracy: 0.9946666666666667\n",
      "Testing loss: 0.0725723628331403\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Training loss: 0.02607918769295017\n",
      "Training accuracy: 0.9951166666666666\n",
      "Testing loss: 0.07152165462423092\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Training loss: 0.02541297235613068\n",
      "Training accuracy: 0.9952666666666666\n",
      "Testing loss: 0.07187859226347416\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Training loss: 0.02487600939149658\n",
      "Training accuracy: 0.9953666666666666\n",
      "Testing loss: 0.0750021615842725\n",
      "Testing accuracy: 0.9766\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Training loss: 0.02399674431420863\n",
      "Training accuracy: 0.9957833333333334\n",
      "Testing loss: 0.07133107160212128\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Training loss: 0.023521975681434076\n",
      "Training accuracy: 0.9960166666666667\n",
      "Testing loss: 0.07240514034868047\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Training loss: 0.022922371410081785\n",
      "Training accuracy: 0.9959333333333333\n",
      "Testing loss: 0.07977176474401383\n",
      "Testing accuracy: 0.976\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Training loss: 0.022412578589717548\n",
      "Training accuracy: 0.9961\n",
      "Testing loss: 0.07211631582753295\n",
      "Testing accuracy: 0.9784\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Training loss: 0.02186428714344899\n",
      "Training accuracy: 0.9963333333333333\n",
      "Testing loss: 0.07264185762092186\n",
      "Testing accuracy: 0.9772\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Training loss: 0.021358636601020893\n",
      "Training accuracy: 0.99655\n",
      "Testing loss: 0.07328070963697306\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Training loss: 0.020770653001219035\n",
      "Training accuracy: 0.9968\n",
      "Testing loss: 0.0706531414969759\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Training loss: 0.020258902272954585\n",
      "Training accuracy: 0.9967833333333334\n",
      "Testing loss: 0.0728411394020151\n",
      "Testing accuracy: 0.9769\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Training loss: 0.019736123644312223\n",
      "Training accuracy: 0.9969333333333333\n",
      "Testing loss: 0.07077090464870499\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Training loss: 0.01922611693603297\n",
      "Training accuracy: 0.9973333333333333\n",
      "Testing loss: 0.07127572573234986\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Training loss: 0.01897334858017663\n",
      "Training accuracy: 0.9974833333333334\n",
      "Testing loss: 0.07269103271637563\n",
      "Testing accuracy: 0.9772\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Training loss: 0.01849977478881677\n",
      "Training accuracy: 0.9974\n",
      "Testing loss: 0.07227416790037114\n",
      "Testing accuracy: 0.977\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Training loss: 0.017995420921345553\n",
      "Training accuracy: 0.9976666666666667\n",
      "Testing loss: 0.07257572986529606\n",
      "Testing accuracy: 0.9776\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Training loss: 0.01755944159689049\n",
      "Training accuracy: 0.9978333333333333\n",
      "Testing loss: 0.07116858854112779\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Training loss: 0.01722882287080089\n",
      "Training accuracy: 0.99775\n",
      "Testing loss: 0.0723557833472421\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Training loss: 0.01688284292444587\n",
      "Training accuracy: 0.99775\n",
      "Testing loss: 0.07251014499361538\n",
      "Testing accuracy: 0.9787\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Training loss: 0.016417717394108574\n",
      "Training accuracy: 0.998\n",
      "Testing loss: 0.07162615629862164\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Training loss: 0.01603912528331081\n",
      "Training accuracy: 0.99815\n",
      "Testing loss: 0.0716710405581435\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Training loss: 0.015673684853315353\n",
      "Training accuracy: 0.9983833333333333\n",
      "Testing loss: 0.07075298414269024\n",
      "Testing accuracy: 0.9773\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Training loss: 0.015313542961205046\n",
      "Training accuracy: 0.9983666666666666\n",
      "Testing loss: 0.07141637272397235\n",
      "Testing accuracy: 0.9786\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Training loss: 0.01504151611191531\n",
      "Training accuracy: 0.9983666666666666\n",
      "Testing loss: 0.07302122750203867\n",
      "Testing accuracy: 0.9776\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Training loss: 0.014736526155720154\n",
      "Training accuracy: 0.9984166666666666\n",
      "Testing loss: 0.0696902251639255\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Training loss: 0.014385401199758053\n",
      "Training accuracy: 0.9984833333333333\n",
      "Testing loss: 0.07141310836568143\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Training loss: 0.01408378987411658\n",
      "Training accuracy: 0.9985833333333334\n",
      "Testing loss: 0.07082575409388418\n",
      "Testing accuracy: 0.978\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Training loss: 0.01391365776496629\n",
      "Training accuracy: 0.9985333333333334\n",
      "Testing loss: 0.07217113111927441\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Training loss: 0.013490055061255892\n",
      "Training accuracy: 0.9987333333333334\n",
      "Testing loss: 0.07222447037299395\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Training loss: 0.013295159890316427\n",
      "Training accuracy: 0.9987833333333334\n",
      "Testing loss: 0.0718084085379745\n",
      "Testing accuracy: 0.978\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Training loss: 0.013036774601911504\n",
      "Training accuracy: 0.9988166666666667\n",
      "Testing loss: 0.07096431074533494\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Training loss: 0.012773270760724941\n",
      "Training accuracy: 0.99885\n",
      "Testing loss: 0.0704099485342195\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Training loss: 0.012454075061281522\n",
      "Training accuracy: 0.9989166666666667\n",
      "Testing loss: 0.07419131074477414\n",
      "Testing accuracy: 0.9772\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Training loss: 0.012222504636086524\n",
      "Training accuracy: 0.999\n",
      "Testing loss: 0.07221401224927206\n",
      "Testing accuracy: 0.978\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Training loss: 0.012050279820462069\n",
      "Training accuracy: 0.9989666666666667\n",
      "Testing loss: 0.07935049038107513\n",
      "Testing accuracy: 0.9755\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Training loss: 0.011791406195114056\n",
      "Training accuracy: 0.9991\n",
      "Testing loss: 0.07064647884308331\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Training loss: 0.011582180770610769\n",
      "Training accuracy: 0.9991\n",
      "Testing loss: 0.07023684330771018\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Training loss: 0.011332092901940148\n",
      "Training accuracy: 0.9990666666666667\n",
      "Testing loss: 0.07235759238350638\n",
      "Testing accuracy: 0.978\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Training loss: 0.011131467388942837\n",
      "Training accuracy: 0.9990666666666667\n",
      "Testing loss: 0.07155882795623082\n",
      "Testing accuracy: 0.9773\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Training loss: 0.010966237336210907\n",
      "Training accuracy: 0.9991333333333333\n",
      "Testing loss: 0.07178008979391069\n",
      "Testing accuracy: 0.978\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Training loss: 0.010809701678839823\n",
      "Training accuracy: 0.9990666666666667\n",
      "Testing loss: 0.0723940679495027\n",
      "Testing accuracy: 0.9776\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Training loss: 0.01059640109160294\n",
      "Training accuracy: 0.9992666666666666\n",
      "Testing loss: 0.07138002270189037\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Training loss: 0.0103256707812349\n",
      "Training accuracy: 0.9993333333333333\n",
      "Testing loss: 0.07148865329629629\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Training loss: 0.010122741307814915\n",
      "Training accuracy: 0.9993666666666666\n",
      "Testing loss: 0.07168842720722032\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Training loss: 0.010001209698741634\n",
      "Training accuracy: 0.9994333333333333\n",
      "Testing loss: 0.07190730833451435\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Training loss: 0.009809430104618272\n",
      "Training accuracy: 0.9994833333333333\n",
      "Testing loss: 0.07376781409861413\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 98\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0096693051956594\n",
      "Training accuracy: 0.9994\n",
      "Testing loss: 0.07252712496467362\n",
      "Testing accuracy: 0.9786\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Training loss: 0.009507118334559103\n",
      "Training accuracy: 0.9995166666666667\n",
      "Testing loss: 0.07246557981254842\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Training loss: 0.009324274179153144\n",
      "Training accuracy: 0.9995333333333334\n",
      "Testing loss: 0.07222756499329665\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Training loss: 0.00922258551158011\n",
      "Training accuracy: 0.99955\n",
      "Testing loss: 0.07099083842039346\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Training loss: 0.009007129730843007\n",
      "Training accuracy: 0.99955\n",
      "Testing loss: 0.07203882252411906\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Training loss: 0.00886084193860491\n",
      "Training accuracy: 0.9995333333333334\n",
      "Testing loss: 0.07225495158801461\n",
      "Testing accuracy: 0.978\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Training loss: 0.008737702565826476\n",
      "Training accuracy: 0.9996\n",
      "Testing loss: 0.07235006815946667\n",
      "Testing accuracy: 0.978\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Training loss: 0.008644282445684076\n",
      "Training accuracy: 0.9997166666666667\n",
      "Testing loss: 0.07709813436973768\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Training loss: 0.008445385699346662\n",
      "Training accuracy: 0.9997166666666667\n",
      "Testing loss: 0.07212777430867141\n",
      "Testing accuracy: 0.9785\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Training loss: 0.0082690479779616\n",
      "Training accuracy: 0.99965\n",
      "Testing loss: 0.07176381807958197\n",
      "Testing accuracy: 0.9788\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Training loss: 0.008173636091469476\n",
      "Training accuracy: 0.9997166666666667\n",
      "Testing loss: 0.07278543392753904\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Training loss: 0.00803638241905719\n",
      "Training accuracy: 0.9997333333333334\n",
      "Testing loss: 0.07313741617509799\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Training loss: 0.007950795170664788\n",
      "Training accuracy: 0.99975\n",
      "Testing loss: 0.0733264302464119\n",
      "Testing accuracy: 0.978\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Training loss: 0.007872397280205041\n",
      "Training accuracy: 0.9998166666666667\n",
      "Testing loss: 0.07285633769478043\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Training loss: 0.0077344604703287285\n",
      "Training accuracy: 0.9997333333333334\n",
      "Testing loss: 0.07160591808923336\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Training loss: 0.0075782736752803125\n",
      "Training accuracy: 0.9997833333333334\n",
      "Testing loss: 0.07320058224782063\n",
      "Testing accuracy: 0.978\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Training loss: 0.00749693930807213\n",
      "Training accuracy: 0.9997833333333334\n",
      "Testing loss: 0.07296673651520685\n",
      "Testing accuracy: 0.978\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Training loss: 0.0073966316527997455\n",
      "Training accuracy: 0.9997833333333334\n",
      "Testing loss: 0.0730757277107065\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Training loss: 0.007290417713361481\n",
      "Training accuracy: 0.9998333333333334\n",
      "Testing loss: 0.07259676603757581\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Training loss: 0.007165730042445163\n",
      "Training accuracy: 0.9998166666666667\n",
      "Testing loss: 0.0733410974610063\n",
      "Testing accuracy: 0.9787\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Training loss: 0.007082293008267879\n",
      "Training accuracy: 0.9998166666666667\n",
      "Testing loss: 0.07567472358168975\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Training loss: 0.006979945343049864\n",
      "Training accuracy: 0.99985\n",
      "Testing loss: 0.07382840672389242\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Training loss: 0.006866855443703632\n",
      "Training accuracy: 0.9998833333333333\n",
      "Testing loss: 0.07386578958270965\n",
      "Testing accuracy: 0.978\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Training loss: 0.006783375405830642\n",
      "Training accuracy: 0.99985\n",
      "Testing loss: 0.07218780570699104\n",
      "Testing accuracy: 0.978\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Training loss: 0.006705393606921037\n",
      "Training accuracy: 0.9999\n",
      "Testing loss: 0.07364348435509731\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Training loss: 0.0065957901541764535\n",
      "Training accuracy: 0.9998833333333333\n",
      "Testing loss: 0.07307661404302285\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Training loss: 0.006528339222601305\n",
      "Training accuracy: 0.99985\n",
      "Testing loss: 0.07398242176329468\n",
      "Testing accuracy: 0.9776\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Training loss: 0.00645363445263356\n",
      "Training accuracy: 0.9999\n",
      "Testing loss: 0.07352090154620256\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Training loss: 0.006366409360182782\n",
      "Training accuracy: 0.9999\n",
      "Testing loss: 0.07271178956974036\n",
      "Testing accuracy: 0.9784\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Training loss: 0.006285547265596688\n",
      "Training accuracy: 0.9999166666666667\n",
      "Testing loss: 0.07371755727050695\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Training loss: 0.006159374620571422\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07347528949442207\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Training loss: 0.00612137025819781\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07427615641503577\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Training loss: 0.006043232561958333\n",
      "Training accuracy: 0.9998666666666667\n",
      "Testing loss: 0.07330135926218692\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Training loss: 0.005964965575033178\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07362694229788272\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Training loss: 0.005909095387222866\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.0732034024690533\n",
      "Testing accuracy: 0.978\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Training loss: 0.005816084269434214\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07365913535942839\n",
      "Testing accuracy: 0.9785\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Training loss: 0.005710894801405569\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.0736601230092228\n",
      "Testing accuracy: 0.9789\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Training loss: 0.005704706728210052\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07363033556278534\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Training loss: 0.005594754570334529\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07380383206897706\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Training loss: 0.005565431064305206\n",
      "Training accuracy: 0.9999166666666667\n",
      "Testing loss: 0.07353169746510833\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Training loss: 0.005468878140983483\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07345674532409283\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Training loss: 0.005410230713772277\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07434358469441912\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Training loss: 0.005356378569453955\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07336329236725986\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Training loss: 0.005290522420344253\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07420023693054303\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Training loss: 0.005241965782828629\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07400989732156348\n",
      "Testing accuracy: 0.9787\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Training loss: 0.005164995361926655\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07378924007647653\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Training loss: 0.005095662221343567\n",
      "Training accuracy: 0.9999666666666667\n",
      "Testing loss: 0.0742827490376973\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Training loss: 0.005072993428229044\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07409450172032912\n",
      "Testing accuracy: 0.9786\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Training loss: 0.005019008654821664\n",
      "Training accuracy: 0.9999333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 0.07428963483640462\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Training loss: 0.004943513346556574\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07377906280620154\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Training loss: 0.004911605197004974\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07399287143541251\n",
      "Testing accuracy: 0.978\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Training loss: 0.004851995894561211\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07437987723402607\n",
      "Testing accuracy: 0.9787\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Training loss: 0.004765998089375595\n",
      "Training accuracy: 0.9999666666666667\n",
      "Testing loss: 0.07489689214619998\n",
      "Testing accuracy: 0.9779\n",
      "Done\n",
      "Total Time Consumed: 2234.873111963272\n"
     ]
    }
   ],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(1) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model1,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model1,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c1032f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(), '../output/ANN/ANN1_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d0bd5",
   "metadata": {},
   "source": [
    "### Seed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa44ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.1 \n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "model2 = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model2.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "312d3311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 0.0047495700763538476\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07491658408280794\n",
      "Testing accuracy: 0.978\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training loss: 0.004693165396464367\n",
      "Training accuracy: 0.9999666666666667\n",
      "Testing loss: 0.07470454824057698\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j9/stxf0ygx7r17jmzqd5vp91dm0000gn/T/ipykernel_1315/3381551095.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {i+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss_record\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtest_loss_record\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/j9/stxf0ygx7r17jmzqd5vp91dm0000gn/T/ipykernel_1315/763263482.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mrunning_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Compute prediction and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \"\"\"\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(2) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model2,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model2,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55da3f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model2.state_dict(), '../output/ANN/ANN2_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171cba33",
   "metadata": {},
   "source": [
    "### Seed 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f49683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.1 \n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "model3 = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model3.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(3) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model3,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model3,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0579d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model3.state_dict(), '../output/ANN/ANN3_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cfdb85",
   "metadata": {},
   "source": [
    "### Seed 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08266a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.1 \n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "model4 = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model4.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(4) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model4,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model4,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83f4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model4.state_dict(), '../output/ANN/ANN4_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73355c3b",
   "metadata": {},
   "source": [
    "### Seed 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cdb927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.1 \n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "model5 = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model5.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(5) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model5,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model5,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69672d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model5.state_dict(), '../output/ANN/ANN5_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d846873",
   "metadata": {},
   "source": [
    "## 3. (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fc62a1",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6629162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.01 \n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "model_lr01 = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_lr01.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01724450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(1) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model_lr01,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model_lr01,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_lr01.state_dict(), '../output/ANN/ANN_lr01_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bda580a",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.2\n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "model_lr2 = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_lr2.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f4e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(1) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model_lr2,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model_lr2,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_lr2.state_dict(), '../output/ANN/ANN_lr2_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e82b591",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.5\n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "model_lr5 = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_lr5.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c782bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(1) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model_lr5,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model_lr5,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb49c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_lr5.state_dict(), '../output/ANN/ANN_lr5_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b27406",
   "metadata": {},
   "source": [
    "### Momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.1 \n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "momentum = 0.5\n",
    "model_m5 = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_m5.parameters(), lr = learning_rate, momentum = momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905554f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(1) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model_m5,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model_m5,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_m5.state_dict(), '../output/ANN/ANN_m5_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43049ec",
   "metadata": {},
   "source": [
    "### Momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9c5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.1 \n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "momentum = 0.9\n",
    "model_m9 = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_m9.parameters(), lr = learning_rate, momentum = momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d566545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(1) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model_m9,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model_m9,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d9d522",
   "metadata": {},
   "source": [
    "## 4. (a) & (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "090563ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Convolutional Neural Network\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cv1 = nn.Conv2d(1, 32, 3)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.cv1(x))\n",
    "        x = F.max_pool2d(x, 3, stride = 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4cc71fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of CNN(\n",
       "  (cv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       ")>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "cnn.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f829a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.1 \n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "model = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc1bb7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    running_loss, correct = 0, 0\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    running_loss = running_loss / size\n",
    "    correct /= size\n",
    "    print('Training loss: {}'.format(running_loss))\n",
    "    print('Training accuracy: {}'.format(correct))\n",
    "    return running_loss, correct\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print('Testing loss: {}'.format(test_loss))\n",
    "    print('Testing accuracy: {}'.format(correct))\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6131c",
   "metadata": {},
   "source": [
    "### Seed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "069e8ab8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 0.6312321174780527\n",
      "Training accuracy: 0.8388166666666667\n",
      "Testing loss: 0.32605285197496414\n",
      "Testing accuracy: 0.9114\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training loss: 0.30747619849443436\n",
      "Training accuracy: 0.9101333333333333\n",
      "Testing loss: 0.2652289485855467\n",
      "Testing accuracy: 0.9222\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Training loss: 0.25936088887055714\n",
      "Training accuracy: 0.9242333333333334\n",
      "Testing loss: 0.2467449258087547\n",
      "Testing accuracy: 0.9275\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Training loss: 0.22716621174017587\n",
      "Training accuracy: 0.9343833333333333\n",
      "Testing loss: 0.2099801887325041\n",
      "Testing accuracy: 0.9393\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Training loss: 0.20135708603461583\n",
      "Training accuracy: 0.94155\n",
      "Testing loss: 0.20063233762315125\n",
      "Testing accuracy: 0.9412\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Training loss: 0.1818713160375754\n",
      "Training accuracy: 0.9471166666666667\n",
      "Testing loss: 0.17156985816160206\n",
      "Testing accuracy: 0.9494\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Training loss: 0.16503312250375748\n",
      "Training accuracy: 0.9521333333333334\n",
      "Testing loss: 0.16374759309610742\n",
      "Testing accuracy: 0.9513\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Training loss: 0.15132120109796524\n",
      "Training accuracy: 0.9564333333333334\n",
      "Testing loss: 0.161040210420159\n",
      "Testing accuracy: 0.9523\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Training loss: 0.13996072180469832\n",
      "Training accuracy: 0.9595166666666667\n",
      "Testing loss: 0.14541404543646202\n",
      "Testing accuracy: 0.9571\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Training loss: 0.13020200232863427\n",
      "Training accuracy: 0.9628166666666667\n",
      "Testing loss: 0.1323606773356723\n",
      "Testing accuracy: 0.9606\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Training loss: 0.12150704482595126\n",
      "Training accuracy: 0.9650166666666666\n",
      "Testing loss: 0.13427240313712957\n",
      "Testing accuracy: 0.9612\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Training loss: 0.11398574720819792\n",
      "Training accuracy: 0.9678166666666667\n",
      "Testing loss: 0.12639727727005814\n",
      "Testing accuracy: 0.9622\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Training loss: 0.10681332801183065\n",
      "Training accuracy: 0.9695666666666667\n",
      "Testing loss: 0.11848506357544547\n",
      "Testing accuracy: 0.9647\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Training loss: 0.10189377796550592\n",
      "Training accuracy: 0.9715\n",
      "Testing loss: 0.11363248516669035\n",
      "Testing accuracy: 0.9664\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Training loss: 0.09621494003931681\n",
      "Training accuracy: 0.97295\n",
      "Testing loss: 0.10666891451996223\n",
      "Testing accuracy: 0.9671\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Training loss: 0.09114216537276904\n",
      "Training accuracy: 0.9746166666666667\n",
      "Testing loss: 0.1047551214552609\n",
      "Testing accuracy: 0.9682\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Training loss: 0.0868209639877081\n",
      "Training accuracy: 0.9759\n",
      "Testing loss: 0.10329806789233806\n",
      "Testing accuracy: 0.9682\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Training loss: 0.08265571415225664\n",
      "Training accuracy: 0.9773166666666666\n",
      "Testing loss: 0.10220716120141327\n",
      "Testing accuracy: 0.9684\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Training loss: 0.07906140153557062\n",
      "Training accuracy: 0.9785333333333334\n",
      "Testing loss: 0.09585777705143782\n",
      "Testing accuracy: 0.9695\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Training loss: 0.07539224013586839\n",
      "Training accuracy: 0.9797833333333333\n",
      "Testing loss: 0.09567331377962593\n",
      "Testing accuracy: 0.9706\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Training loss: 0.07229305909276008\n",
      "Training accuracy: 0.9802333333333333\n",
      "Testing loss: 0.09714907073196333\n",
      "Testing accuracy: 0.97\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Training loss: 0.06903246783216795\n",
      "Training accuracy: 0.98145\n",
      "Testing loss: 0.0997185023726931\n",
      "Testing accuracy: 0.9709\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Training loss: 0.06638706729362408\n",
      "Training accuracy: 0.98225\n",
      "Testing loss: 0.09130564043713603\n",
      "Testing accuracy: 0.9715\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Training loss: 0.0635785830527544\n",
      "Training accuracy: 0.9829333333333333\n",
      "Testing loss: 0.09025384842234242\n",
      "Testing accuracy: 0.9732\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Training loss: 0.06127839450240135\n",
      "Training accuracy: 0.9836166666666667\n",
      "Testing loss: 0.08533021614287689\n",
      "Testing accuracy: 0.9742\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Training loss: 0.05905410729100307\n",
      "Training accuracy: 0.9846333333333334\n",
      "Testing loss: 0.08461978217100452\n",
      "Testing accuracy: 0.9739\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Training loss: 0.056904677504797774\n",
      "Training accuracy: 0.9852666666666666\n",
      "Testing loss: 0.08305285317574147\n",
      "Testing accuracy: 0.9738\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Training loss: 0.054453953233857946\n",
      "Training accuracy: 0.9860333333333333\n",
      "Testing loss: 0.08625281558840707\n",
      "Testing accuracy: 0.9728\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Training loss: 0.05274626832405726\n",
      "Training accuracy: 0.9863333333333333\n",
      "Testing loss: 0.08491130966906714\n",
      "Testing accuracy: 0.9743\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Training loss: 0.050896600427230196\n",
      "Training accuracy: 0.9871666666666666\n",
      "Testing loss: 0.0833016288211676\n",
      "Testing accuracy: 0.9753\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Training loss: 0.04903690692037344\n",
      "Training accuracy: 0.9878166666666667\n",
      "Testing loss: 0.08196763690727152\n",
      "Testing accuracy: 0.9743\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Training loss: 0.04759960208584865\n",
      "Training accuracy: 0.9881166666666666\n",
      "Testing loss: 0.08144234039577519\n",
      "Testing accuracy: 0.9736\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Training loss: 0.04606730470061302\n",
      "Training accuracy: 0.9883833333333333\n",
      "Testing loss: 0.07986198049216608\n",
      "Testing accuracy: 0.975\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Training loss: 0.04455727094064156\n",
      "Training accuracy: 0.989\n",
      "Testing loss: 0.08772501382371707\n",
      "Testing accuracy: 0.9737\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Training loss: 0.04286884162972371\n",
      "Training accuracy: 0.98965\n",
      "Testing loss: 0.08790081407234168\n",
      "Testing accuracy: 0.9737\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Training loss: 0.0414949409176906\n",
      "Training accuracy: 0.9901833333333333\n",
      "Testing loss: 0.07857880550347696\n",
      "Testing accuracy: 0.9757\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Training loss: 0.04023696773697932\n",
      "Training accuracy: 0.9905833333333334\n",
      "Testing loss: 0.0784410725816325\n",
      "Testing accuracy: 0.9758\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Training loss: 0.03915539190073808\n",
      "Training accuracy: 0.9908333333333333\n",
      "Testing loss: 0.07386640872758855\n",
      "Testing accuracy: 0.9776\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Training loss: 0.03779750686635574\n",
      "Training accuracy: 0.9914\n",
      "Testing loss: 0.07584405345189724\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Training loss: 0.03672922372023264\n",
      "Training accuracy: 0.99175\n",
      "Testing loss: 0.074365292785535\n",
      "Testing accuracy: 0.9763\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Training loss: 0.0358541913792491\n",
      "Training accuracy: 0.9916833333333334\n",
      "Testing loss: 0.07623604670238153\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Training loss: 0.03468198499704401\n",
      "Training accuracy: 0.9919333333333333\n",
      "Testing loss: 0.07358820981399458\n",
      "Testing accuracy: 0.9772\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Training loss: 0.033600581060349945\n",
      "Training accuracy: 0.99245\n",
      "Testing loss: 0.07306452501553354\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Training loss: 0.03276625404109557\n",
      "Training accuracy: 0.9927333333333334\n",
      "Testing loss: 0.07296618964422233\n",
      "Testing accuracy: 0.9766\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Training loss: 0.03185164739266038\n",
      "Training accuracy: 0.9931333333333333\n",
      "Testing loss: 0.0759282445351172\n",
      "Testing accuracy: 0.9755\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Training loss: 0.030896728436648847\n",
      "Training accuracy: 0.9934\n",
      "Testing loss: 0.07313415110941715\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Training loss: 0.029929666263858477\n",
      "Training accuracy: 0.9937666666666667\n",
      "Testing loss: 0.07394555885237968\n",
      "Testing accuracy: 0.9772\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Training loss: 0.02937405418281754\n",
      "Training accuracy: 0.9939833333333333\n",
      "Testing loss: 0.07374226104669795\n",
      "Testing accuracy: 0.9769\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Training loss: 0.02826606729204456\n",
      "Training accuracy: 0.9942833333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 0.07407478763941366\n",
      "Testing accuracy: 0.9773\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Training loss: 0.02773880961301426\n",
      "Training accuracy: 0.99435\n",
      "Testing loss: 0.07299135281010323\n",
      "Testing accuracy: 0.977\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Training loss: 0.026979105322311323\n",
      "Training accuracy: 0.9946666666666667\n",
      "Testing loss: 0.0725723628331403\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Training loss: 0.02607918769295017\n",
      "Training accuracy: 0.9951166666666666\n",
      "Testing loss: 0.07152165462423092\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Training loss: 0.02541297235613068\n",
      "Training accuracy: 0.9952666666666666\n",
      "Testing loss: 0.07187859226347416\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Training loss: 0.02487600939149658\n",
      "Training accuracy: 0.9953666666666666\n",
      "Testing loss: 0.0750021615842725\n",
      "Testing accuracy: 0.9766\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Training loss: 0.02399674431420863\n",
      "Training accuracy: 0.9957833333333334\n",
      "Testing loss: 0.07133107160212128\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Training loss: 0.023521975681434076\n",
      "Training accuracy: 0.9960166666666667\n",
      "Testing loss: 0.07240514034868047\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Training loss: 0.022922371410081785\n",
      "Training accuracy: 0.9959333333333333\n",
      "Testing loss: 0.07977176474401383\n",
      "Testing accuracy: 0.976\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Training loss: 0.022412578589717548\n",
      "Training accuracy: 0.9961\n",
      "Testing loss: 0.07211631582753295\n",
      "Testing accuracy: 0.9784\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Training loss: 0.02186428714344899\n",
      "Training accuracy: 0.9963333333333333\n",
      "Testing loss: 0.07264185762092186\n",
      "Testing accuracy: 0.9772\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Training loss: 0.021358636601020893\n",
      "Training accuracy: 0.99655\n",
      "Testing loss: 0.07328070963697306\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Training loss: 0.020770653001219035\n",
      "Training accuracy: 0.9968\n",
      "Testing loss: 0.0706531414969759\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Training loss: 0.020258902272954585\n",
      "Training accuracy: 0.9967833333333334\n",
      "Testing loss: 0.0728411394020151\n",
      "Testing accuracy: 0.9769\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Training loss: 0.019736123644312223\n",
      "Training accuracy: 0.9969333333333333\n",
      "Testing loss: 0.07077090464870499\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Training loss: 0.01922611693603297\n",
      "Training accuracy: 0.9973333333333333\n",
      "Testing loss: 0.07127572573234986\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Training loss: 0.01897334858017663\n",
      "Training accuracy: 0.9974833333333334\n",
      "Testing loss: 0.07269103271637563\n",
      "Testing accuracy: 0.9772\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Training loss: 0.01849977478881677\n",
      "Training accuracy: 0.9974\n",
      "Testing loss: 0.07227416790037114\n",
      "Testing accuracy: 0.977\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Training loss: 0.017995420921345553\n",
      "Training accuracy: 0.9976666666666667\n",
      "Testing loss: 0.07257572986529606\n",
      "Testing accuracy: 0.9776\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Training loss: 0.01755944159689049\n",
      "Training accuracy: 0.9978333333333333\n",
      "Testing loss: 0.07116858854112779\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Training loss: 0.01722882287080089\n",
      "Training accuracy: 0.99775\n",
      "Testing loss: 0.0723557833472421\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Training loss: 0.01688284292444587\n",
      "Training accuracy: 0.99775\n",
      "Testing loss: 0.07251014499361538\n",
      "Testing accuracy: 0.9787\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Training loss: 0.016417717394108574\n",
      "Training accuracy: 0.998\n",
      "Testing loss: 0.07162615629862164\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Training loss: 0.01603912528331081\n",
      "Training accuracy: 0.99815\n",
      "Testing loss: 0.0716710405581435\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Training loss: 0.015673684853315353\n",
      "Training accuracy: 0.9983833333333333\n",
      "Testing loss: 0.07075298414269024\n",
      "Testing accuracy: 0.9773\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Training loss: 0.015313542961205046\n",
      "Training accuracy: 0.9983666666666666\n",
      "Testing loss: 0.07141637272397235\n",
      "Testing accuracy: 0.9786\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Training loss: 0.01504151611191531\n",
      "Training accuracy: 0.9983666666666666\n",
      "Testing loss: 0.07302122750203867\n",
      "Testing accuracy: 0.9776\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Training loss: 0.014736526155720154\n",
      "Training accuracy: 0.9984166666666666\n",
      "Testing loss: 0.0696902251639255\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Training loss: 0.014385401199758053\n",
      "Training accuracy: 0.9984833333333333\n",
      "Testing loss: 0.07141310836568143\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Training loss: 0.01408378987411658\n",
      "Training accuracy: 0.9985833333333334\n",
      "Testing loss: 0.07082575409388418\n",
      "Testing accuracy: 0.978\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Training loss: 0.01391365776496629\n",
      "Training accuracy: 0.9985333333333334\n",
      "Testing loss: 0.07217113111927441\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Training loss: 0.013490055061255892\n",
      "Training accuracy: 0.9987333333333334\n",
      "Testing loss: 0.07222447037299395\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Training loss: 0.013295159890316427\n",
      "Training accuracy: 0.9987833333333334\n",
      "Testing loss: 0.0718084085379745\n",
      "Testing accuracy: 0.978\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Training loss: 0.013036774601911504\n",
      "Training accuracy: 0.9988166666666667\n",
      "Testing loss: 0.07096431074533494\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Training loss: 0.012773270760724941\n",
      "Training accuracy: 0.99885\n",
      "Testing loss: 0.0704099485342195\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Training loss: 0.012454075061281522\n",
      "Training accuracy: 0.9989166666666667\n",
      "Testing loss: 0.07419131074477414\n",
      "Testing accuracy: 0.9772\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Training loss: 0.012222504636086524\n",
      "Training accuracy: 0.999\n",
      "Testing loss: 0.07221401224927206\n",
      "Testing accuracy: 0.978\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Training loss: 0.012050279820462069\n",
      "Training accuracy: 0.9989666666666667\n",
      "Testing loss: 0.07935049038107513\n",
      "Testing accuracy: 0.9755\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Training loss: 0.011791406195114056\n",
      "Training accuracy: 0.9991\n",
      "Testing loss: 0.07064647884308331\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Training loss: 0.011582180770610769\n",
      "Training accuracy: 0.9991\n",
      "Testing loss: 0.07023684330771018\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Training loss: 0.011332092901940148\n",
      "Training accuracy: 0.9990666666666667\n",
      "Testing loss: 0.07235759238350638\n",
      "Testing accuracy: 0.978\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Training loss: 0.011131467388942837\n",
      "Training accuracy: 0.9990666666666667\n",
      "Testing loss: 0.07155882795623082\n",
      "Testing accuracy: 0.9773\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Training loss: 0.010966237336210907\n",
      "Training accuracy: 0.9991333333333333\n",
      "Testing loss: 0.07178008979391069\n",
      "Testing accuracy: 0.978\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Training loss: 0.010809701678839823\n",
      "Training accuracy: 0.9990666666666667\n",
      "Testing loss: 0.0723940679495027\n",
      "Testing accuracy: 0.9776\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Training loss: 0.01059640109160294\n",
      "Training accuracy: 0.9992666666666666\n",
      "Testing loss: 0.07138002270189037\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Training loss: 0.0103256707812349\n",
      "Training accuracy: 0.9993333333333333\n",
      "Testing loss: 0.07148865329629629\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Training loss: 0.010122741307814915\n",
      "Training accuracy: 0.9993666666666666\n",
      "Testing loss: 0.07168842720722032\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Training loss: 0.010001209698741634\n",
      "Training accuracy: 0.9994333333333333\n",
      "Testing loss: 0.07190730833451435\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Training loss: 0.009809430104618272\n",
      "Training accuracy: 0.9994833333333333\n",
      "Testing loss: 0.07376781409861413\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 98\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0096693051956594\n",
      "Training accuracy: 0.9994\n",
      "Testing loss: 0.07252712496467362\n",
      "Testing accuracy: 0.9786\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Training loss: 0.009507118334559103\n",
      "Training accuracy: 0.9995166666666667\n",
      "Testing loss: 0.07246557981254842\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Training loss: 0.009324274179153144\n",
      "Training accuracy: 0.9995333333333334\n",
      "Testing loss: 0.07222756499329665\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Training loss: 0.00922258551158011\n",
      "Training accuracy: 0.99955\n",
      "Testing loss: 0.07099083842039346\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Training loss: 0.009007129730843007\n",
      "Training accuracy: 0.99955\n",
      "Testing loss: 0.07203882252411906\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Training loss: 0.00886084193860491\n",
      "Training accuracy: 0.9995333333333334\n",
      "Testing loss: 0.07225495158801461\n",
      "Testing accuracy: 0.978\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Training loss: 0.008737702565826476\n",
      "Training accuracy: 0.9996\n",
      "Testing loss: 0.07235006815946667\n",
      "Testing accuracy: 0.978\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Training loss: 0.008644282445684076\n",
      "Training accuracy: 0.9997166666666667\n",
      "Testing loss: 0.07709813436973768\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Training loss: 0.008445385699346662\n",
      "Training accuracy: 0.9997166666666667\n",
      "Testing loss: 0.07212777430867141\n",
      "Testing accuracy: 0.9785\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Training loss: 0.0082690479779616\n",
      "Training accuracy: 0.99965\n",
      "Testing loss: 0.07176381807958197\n",
      "Testing accuracy: 0.9788\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Training loss: 0.008173636091469476\n",
      "Training accuracy: 0.9997166666666667\n",
      "Testing loss: 0.07278543392753904\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Training loss: 0.00803638241905719\n",
      "Training accuracy: 0.9997333333333334\n",
      "Testing loss: 0.07313741617509799\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Training loss: 0.007950795170664788\n",
      "Training accuracy: 0.99975\n",
      "Testing loss: 0.0733264302464119\n",
      "Testing accuracy: 0.978\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Training loss: 0.007872397280205041\n",
      "Training accuracy: 0.9998166666666667\n",
      "Testing loss: 0.07285633769478043\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Training loss: 0.0077344604703287285\n",
      "Training accuracy: 0.9997333333333334\n",
      "Testing loss: 0.07160591808923336\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Training loss: 0.0075782736752803125\n",
      "Training accuracy: 0.9997833333333334\n",
      "Testing loss: 0.07320058224782063\n",
      "Testing accuracy: 0.978\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Training loss: 0.00749693930807213\n",
      "Training accuracy: 0.9997833333333334\n",
      "Testing loss: 0.07296673651520685\n",
      "Testing accuracy: 0.978\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Training loss: 0.0073966316527997455\n",
      "Training accuracy: 0.9997833333333334\n",
      "Testing loss: 0.0730757277107065\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Training loss: 0.007290417713361481\n",
      "Training accuracy: 0.9998333333333334\n",
      "Testing loss: 0.07259676603757581\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Training loss: 0.007165730042445163\n",
      "Training accuracy: 0.9998166666666667\n",
      "Testing loss: 0.0733410974610063\n",
      "Testing accuracy: 0.9787\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Training loss: 0.007082293008267879\n",
      "Training accuracy: 0.9998166666666667\n",
      "Testing loss: 0.07567472358168975\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Training loss: 0.006979945343049864\n",
      "Training accuracy: 0.99985\n",
      "Testing loss: 0.07382840672389242\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Training loss: 0.006866855443703632\n",
      "Training accuracy: 0.9998833333333333\n",
      "Testing loss: 0.07386578958270965\n",
      "Testing accuracy: 0.978\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Training loss: 0.006783375405830642\n",
      "Training accuracy: 0.99985\n",
      "Testing loss: 0.07218780570699104\n",
      "Testing accuracy: 0.978\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Training loss: 0.006705393606921037\n",
      "Training accuracy: 0.9999\n",
      "Testing loss: 0.07364348435509731\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Training loss: 0.0065957901541764535\n",
      "Training accuracy: 0.9998833333333333\n",
      "Testing loss: 0.07307661404302285\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Training loss: 0.006528339222601305\n",
      "Training accuracy: 0.99985\n",
      "Testing loss: 0.07398242176329468\n",
      "Testing accuracy: 0.9776\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Training loss: 0.00645363445263356\n",
      "Training accuracy: 0.9999\n",
      "Testing loss: 0.07352090154620256\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Training loss: 0.006366409360182782\n",
      "Training accuracy: 0.9999\n",
      "Testing loss: 0.07271178956974036\n",
      "Testing accuracy: 0.9784\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Training loss: 0.006285547265596688\n",
      "Training accuracy: 0.9999166666666667\n",
      "Testing loss: 0.07371755727050695\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Training loss: 0.006159374620571422\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07347528949442207\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Training loss: 0.00612137025819781\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07427615641503577\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Training loss: 0.006043232561958333\n",
      "Training accuracy: 0.9998666666666667\n",
      "Testing loss: 0.07330135926218692\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Training loss: 0.005964965575033178\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07362694229788272\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Training loss: 0.005909095387222866\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.0732034024690533\n",
      "Testing accuracy: 0.978\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Training loss: 0.005816084269434214\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07365913535942839\n",
      "Testing accuracy: 0.9785\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Training loss: 0.005710894801405569\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.0736601230092228\n",
      "Testing accuracy: 0.9789\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Training loss: 0.005704706728210052\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07363033556278534\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Training loss: 0.005594754570334529\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07380383206897706\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Training loss: 0.005565431064305206\n",
      "Training accuracy: 0.9999166666666667\n",
      "Testing loss: 0.07353169746510833\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Training loss: 0.005468878140983483\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07345674532409283\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Training loss: 0.005410230713772277\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07434358469441912\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Training loss: 0.005356378569453955\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07336329236725986\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Training loss: 0.005290522420344253\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07420023693054303\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Training loss: 0.005241965782828629\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07400989732156348\n",
      "Testing accuracy: 0.9787\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Training loss: 0.005164995361926655\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07378924007647653\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Training loss: 0.005095662221343567\n",
      "Training accuracy: 0.9999666666666667\n",
      "Testing loss: 0.0742827490376973\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Training loss: 0.005072993428229044\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07409450172032912\n",
      "Testing accuracy: 0.9786\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Training loss: 0.005019008654821664\n",
      "Training accuracy: 0.9999333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 0.07428963483640462\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Training loss: 0.004943513346556574\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07377906280620154\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Training loss: 0.004911605197004974\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07399287143541251\n",
      "Testing accuracy: 0.978\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Training loss: 0.004851995894561211\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07437987723402607\n",
      "Testing accuracy: 0.9787\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Training loss: 0.004765998089375595\n",
      "Training accuracy: 0.9999666666666667\n",
      "Testing loss: 0.07489689214619998\n",
      "Testing accuracy: 0.9779\n",
      "Done\n",
      "Total Time Consumed: 2234.873111963272\n"
     ]
    }
   ],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(1) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07b93245",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'net_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50e407",
   "metadata": {},
   "source": [
    "### Seed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e3f95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 0.0047495700763538476\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07491658408280794\n",
      "Testing accuracy: 0.978\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training loss: 0.004693165396464367\n",
      "Training accuracy: 0.9999666666666667\n",
      "Testing loss: 0.07470454824057698\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j9/stxf0ygx7r17jmzqd5vp91dm0000gn/T/ipykernel_1315/3381551095.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {i+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss_record\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtest_loss_record\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/j9/stxf0ygx7r17jmzqd5vp91dm0000gn/T/ipykernel_1315/763263482.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mrunning_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Compute prediction and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \"\"\"\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(2) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba899381",
   "metadata": {},
   "source": [
    "### Seed 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(3) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307f442c",
   "metadata": {},
   "source": [
    "### Seed 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b670f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(4) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829be3d7",
   "metadata": {},
   "source": [
    "### Seed 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccbbc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(5) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea220aa1",
   "metadata": {},
   "source": [
    "## 3. (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1c883",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96584fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.01 \n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "model = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2109a912",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e730ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.2\n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "model = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8470f94f",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5482e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.5\n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "model = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd194c7",
   "metadata": {},
   "source": [
    "### Momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290e0ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.1 \n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "momentum = 0.5\n",
    "model = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum = momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d812d7fc",
   "metadata": {},
   "source": [
    "### Momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf1700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.1 \n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "momentum = 0.9\n",
    "model = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum = momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f22c9e",
   "metadata": {},
   "source": [
    "## 5. (a) & (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0db7ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Fully Connected Neural Network\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ANN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(input_size, 100)\n",
    "        self.layer2 = nn.Linear(100, num_classes)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.sigmoid(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20141d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.1 \n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "model = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc1bb7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    running_loss, correct = 0, 0\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    running_loss = running_loss / size\n",
    "    correct /= size\n",
    "    print('Training loss: {}'.format(running_loss))\n",
    "    print('Training accuracy: {}'.format(correct))\n",
    "    return running_loss, correct\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print('Testing loss: {}'.format(test_loss))\n",
    "    print('Testing accuracy: {}'.format(correct))\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a471511",
   "metadata": {},
   "source": [
    "### Seed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "069e8ab8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 0.6312321174780527\n",
      "Training accuracy: 0.8388166666666667\n",
      "Testing loss: 0.32605285197496414\n",
      "Testing accuracy: 0.9114\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training loss: 0.30747619849443436\n",
      "Training accuracy: 0.9101333333333333\n",
      "Testing loss: 0.2652289485855467\n",
      "Testing accuracy: 0.9222\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Training loss: 0.25936088887055714\n",
      "Training accuracy: 0.9242333333333334\n",
      "Testing loss: 0.2467449258087547\n",
      "Testing accuracy: 0.9275\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Training loss: 0.22716621174017587\n",
      "Training accuracy: 0.9343833333333333\n",
      "Testing loss: 0.2099801887325041\n",
      "Testing accuracy: 0.9393\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Training loss: 0.20135708603461583\n",
      "Training accuracy: 0.94155\n",
      "Testing loss: 0.20063233762315125\n",
      "Testing accuracy: 0.9412\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Training loss: 0.1818713160375754\n",
      "Training accuracy: 0.9471166666666667\n",
      "Testing loss: 0.17156985816160206\n",
      "Testing accuracy: 0.9494\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Training loss: 0.16503312250375748\n",
      "Training accuracy: 0.9521333333333334\n",
      "Testing loss: 0.16374759309610742\n",
      "Testing accuracy: 0.9513\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Training loss: 0.15132120109796524\n",
      "Training accuracy: 0.9564333333333334\n",
      "Testing loss: 0.161040210420159\n",
      "Testing accuracy: 0.9523\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Training loss: 0.13996072180469832\n",
      "Training accuracy: 0.9595166666666667\n",
      "Testing loss: 0.14541404543646202\n",
      "Testing accuracy: 0.9571\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Training loss: 0.13020200232863427\n",
      "Training accuracy: 0.9628166666666667\n",
      "Testing loss: 0.1323606773356723\n",
      "Testing accuracy: 0.9606\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Training loss: 0.12150704482595126\n",
      "Training accuracy: 0.9650166666666666\n",
      "Testing loss: 0.13427240313712957\n",
      "Testing accuracy: 0.9612\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Training loss: 0.11398574720819792\n",
      "Training accuracy: 0.9678166666666667\n",
      "Testing loss: 0.12639727727005814\n",
      "Testing accuracy: 0.9622\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Training loss: 0.10681332801183065\n",
      "Training accuracy: 0.9695666666666667\n",
      "Testing loss: 0.11848506357544547\n",
      "Testing accuracy: 0.9647\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Training loss: 0.10189377796550592\n",
      "Training accuracy: 0.9715\n",
      "Testing loss: 0.11363248516669035\n",
      "Testing accuracy: 0.9664\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Training loss: 0.09621494003931681\n",
      "Training accuracy: 0.97295\n",
      "Testing loss: 0.10666891451996223\n",
      "Testing accuracy: 0.9671\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Training loss: 0.09114216537276904\n",
      "Training accuracy: 0.9746166666666667\n",
      "Testing loss: 0.1047551214552609\n",
      "Testing accuracy: 0.9682\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Training loss: 0.0868209639877081\n",
      "Training accuracy: 0.9759\n",
      "Testing loss: 0.10329806789233806\n",
      "Testing accuracy: 0.9682\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Training loss: 0.08265571415225664\n",
      "Training accuracy: 0.9773166666666666\n",
      "Testing loss: 0.10220716120141327\n",
      "Testing accuracy: 0.9684\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Training loss: 0.07906140153557062\n",
      "Training accuracy: 0.9785333333333334\n",
      "Testing loss: 0.09585777705143782\n",
      "Testing accuracy: 0.9695\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Training loss: 0.07539224013586839\n",
      "Training accuracy: 0.9797833333333333\n",
      "Testing loss: 0.09567331377962593\n",
      "Testing accuracy: 0.9706\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Training loss: 0.07229305909276008\n",
      "Training accuracy: 0.9802333333333333\n",
      "Testing loss: 0.09714907073196333\n",
      "Testing accuracy: 0.97\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Training loss: 0.06903246783216795\n",
      "Training accuracy: 0.98145\n",
      "Testing loss: 0.0997185023726931\n",
      "Testing accuracy: 0.9709\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Training loss: 0.06638706729362408\n",
      "Training accuracy: 0.98225\n",
      "Testing loss: 0.09130564043713603\n",
      "Testing accuracy: 0.9715\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Training loss: 0.0635785830527544\n",
      "Training accuracy: 0.9829333333333333\n",
      "Testing loss: 0.09025384842234242\n",
      "Testing accuracy: 0.9732\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Training loss: 0.06127839450240135\n",
      "Training accuracy: 0.9836166666666667\n",
      "Testing loss: 0.08533021614287689\n",
      "Testing accuracy: 0.9742\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Training loss: 0.05905410729100307\n",
      "Training accuracy: 0.9846333333333334\n",
      "Testing loss: 0.08461978217100452\n",
      "Testing accuracy: 0.9739\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Training loss: 0.056904677504797774\n",
      "Training accuracy: 0.9852666666666666\n",
      "Testing loss: 0.08305285317574147\n",
      "Testing accuracy: 0.9738\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Training loss: 0.054453953233857946\n",
      "Training accuracy: 0.9860333333333333\n",
      "Testing loss: 0.08625281558840707\n",
      "Testing accuracy: 0.9728\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Training loss: 0.05274626832405726\n",
      "Training accuracy: 0.9863333333333333\n",
      "Testing loss: 0.08491130966906714\n",
      "Testing accuracy: 0.9743\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Training loss: 0.050896600427230196\n",
      "Training accuracy: 0.9871666666666666\n",
      "Testing loss: 0.0833016288211676\n",
      "Testing accuracy: 0.9753\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Training loss: 0.04903690692037344\n",
      "Training accuracy: 0.9878166666666667\n",
      "Testing loss: 0.08196763690727152\n",
      "Testing accuracy: 0.9743\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Training loss: 0.04759960208584865\n",
      "Training accuracy: 0.9881166666666666\n",
      "Testing loss: 0.08144234039577519\n",
      "Testing accuracy: 0.9736\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Training loss: 0.04606730470061302\n",
      "Training accuracy: 0.9883833333333333\n",
      "Testing loss: 0.07986198049216608\n",
      "Testing accuracy: 0.975\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Training loss: 0.04455727094064156\n",
      "Training accuracy: 0.989\n",
      "Testing loss: 0.08772501382371707\n",
      "Testing accuracy: 0.9737\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Training loss: 0.04286884162972371\n",
      "Training accuracy: 0.98965\n",
      "Testing loss: 0.08790081407234168\n",
      "Testing accuracy: 0.9737\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Training loss: 0.0414949409176906\n",
      "Training accuracy: 0.9901833333333333\n",
      "Testing loss: 0.07857880550347696\n",
      "Testing accuracy: 0.9757\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Training loss: 0.04023696773697932\n",
      "Training accuracy: 0.9905833333333334\n",
      "Testing loss: 0.0784410725816325\n",
      "Testing accuracy: 0.9758\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Training loss: 0.03915539190073808\n",
      "Training accuracy: 0.9908333333333333\n",
      "Testing loss: 0.07386640872758855\n",
      "Testing accuracy: 0.9776\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Training loss: 0.03779750686635574\n",
      "Training accuracy: 0.9914\n",
      "Testing loss: 0.07584405345189724\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Training loss: 0.03672922372023264\n",
      "Training accuracy: 0.99175\n",
      "Testing loss: 0.074365292785535\n",
      "Testing accuracy: 0.9763\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Training loss: 0.0358541913792491\n",
      "Training accuracy: 0.9916833333333334\n",
      "Testing loss: 0.07623604670238153\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Training loss: 0.03468198499704401\n",
      "Training accuracy: 0.9919333333333333\n",
      "Testing loss: 0.07358820981399458\n",
      "Testing accuracy: 0.9772\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Training loss: 0.033600581060349945\n",
      "Training accuracy: 0.99245\n",
      "Testing loss: 0.07306452501553354\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Training loss: 0.03276625404109557\n",
      "Training accuracy: 0.9927333333333334\n",
      "Testing loss: 0.07296618964422233\n",
      "Testing accuracy: 0.9766\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Training loss: 0.03185164739266038\n",
      "Training accuracy: 0.9931333333333333\n",
      "Testing loss: 0.0759282445351172\n",
      "Testing accuracy: 0.9755\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Training loss: 0.030896728436648847\n",
      "Training accuracy: 0.9934\n",
      "Testing loss: 0.07313415110941715\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Training loss: 0.029929666263858477\n",
      "Training accuracy: 0.9937666666666667\n",
      "Testing loss: 0.07394555885237968\n",
      "Testing accuracy: 0.9772\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Training loss: 0.02937405418281754\n",
      "Training accuracy: 0.9939833333333333\n",
      "Testing loss: 0.07374226104669795\n",
      "Testing accuracy: 0.9769\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Training loss: 0.02826606729204456\n",
      "Training accuracy: 0.9942833333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 0.07407478763941366\n",
      "Testing accuracy: 0.9773\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Training loss: 0.02773880961301426\n",
      "Training accuracy: 0.99435\n",
      "Testing loss: 0.07299135281010323\n",
      "Testing accuracy: 0.977\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Training loss: 0.026979105322311323\n",
      "Training accuracy: 0.9946666666666667\n",
      "Testing loss: 0.0725723628331403\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Training loss: 0.02607918769295017\n",
      "Training accuracy: 0.9951166666666666\n",
      "Testing loss: 0.07152165462423092\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Training loss: 0.02541297235613068\n",
      "Training accuracy: 0.9952666666666666\n",
      "Testing loss: 0.07187859226347416\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Training loss: 0.02487600939149658\n",
      "Training accuracy: 0.9953666666666666\n",
      "Testing loss: 0.0750021615842725\n",
      "Testing accuracy: 0.9766\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Training loss: 0.02399674431420863\n",
      "Training accuracy: 0.9957833333333334\n",
      "Testing loss: 0.07133107160212128\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Training loss: 0.023521975681434076\n",
      "Training accuracy: 0.9960166666666667\n",
      "Testing loss: 0.07240514034868047\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Training loss: 0.022922371410081785\n",
      "Training accuracy: 0.9959333333333333\n",
      "Testing loss: 0.07977176474401383\n",
      "Testing accuracy: 0.976\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Training loss: 0.022412578589717548\n",
      "Training accuracy: 0.9961\n",
      "Testing loss: 0.07211631582753295\n",
      "Testing accuracy: 0.9784\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Training loss: 0.02186428714344899\n",
      "Training accuracy: 0.9963333333333333\n",
      "Testing loss: 0.07264185762092186\n",
      "Testing accuracy: 0.9772\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Training loss: 0.021358636601020893\n",
      "Training accuracy: 0.99655\n",
      "Testing loss: 0.07328070963697306\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Training loss: 0.020770653001219035\n",
      "Training accuracy: 0.9968\n",
      "Testing loss: 0.0706531414969759\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Training loss: 0.020258902272954585\n",
      "Training accuracy: 0.9967833333333334\n",
      "Testing loss: 0.0728411394020151\n",
      "Testing accuracy: 0.9769\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Training loss: 0.019736123644312223\n",
      "Training accuracy: 0.9969333333333333\n",
      "Testing loss: 0.07077090464870499\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Training loss: 0.01922611693603297\n",
      "Training accuracy: 0.9973333333333333\n",
      "Testing loss: 0.07127572573234986\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Training loss: 0.01897334858017663\n",
      "Training accuracy: 0.9974833333333334\n",
      "Testing loss: 0.07269103271637563\n",
      "Testing accuracy: 0.9772\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Training loss: 0.01849977478881677\n",
      "Training accuracy: 0.9974\n",
      "Testing loss: 0.07227416790037114\n",
      "Testing accuracy: 0.977\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Training loss: 0.017995420921345553\n",
      "Training accuracy: 0.9976666666666667\n",
      "Testing loss: 0.07257572986529606\n",
      "Testing accuracy: 0.9776\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Training loss: 0.01755944159689049\n",
      "Training accuracy: 0.9978333333333333\n",
      "Testing loss: 0.07116858854112779\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Training loss: 0.01722882287080089\n",
      "Training accuracy: 0.99775\n",
      "Testing loss: 0.0723557833472421\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Training loss: 0.01688284292444587\n",
      "Training accuracy: 0.99775\n",
      "Testing loss: 0.07251014499361538\n",
      "Testing accuracy: 0.9787\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Training loss: 0.016417717394108574\n",
      "Training accuracy: 0.998\n",
      "Testing loss: 0.07162615629862164\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Training loss: 0.01603912528331081\n",
      "Training accuracy: 0.99815\n",
      "Testing loss: 0.0716710405581435\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Training loss: 0.015673684853315353\n",
      "Training accuracy: 0.9983833333333333\n",
      "Testing loss: 0.07075298414269024\n",
      "Testing accuracy: 0.9773\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Training loss: 0.015313542961205046\n",
      "Training accuracy: 0.9983666666666666\n",
      "Testing loss: 0.07141637272397235\n",
      "Testing accuracy: 0.9786\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Training loss: 0.01504151611191531\n",
      "Training accuracy: 0.9983666666666666\n",
      "Testing loss: 0.07302122750203867\n",
      "Testing accuracy: 0.9776\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Training loss: 0.014736526155720154\n",
      "Training accuracy: 0.9984166666666666\n",
      "Testing loss: 0.0696902251639255\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Training loss: 0.014385401199758053\n",
      "Training accuracy: 0.9984833333333333\n",
      "Testing loss: 0.07141310836568143\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Training loss: 0.01408378987411658\n",
      "Training accuracy: 0.9985833333333334\n",
      "Testing loss: 0.07082575409388418\n",
      "Testing accuracy: 0.978\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Training loss: 0.01391365776496629\n",
      "Training accuracy: 0.9985333333333334\n",
      "Testing loss: 0.07217113111927441\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Training loss: 0.013490055061255892\n",
      "Training accuracy: 0.9987333333333334\n",
      "Testing loss: 0.07222447037299395\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Training loss: 0.013295159890316427\n",
      "Training accuracy: 0.9987833333333334\n",
      "Testing loss: 0.0718084085379745\n",
      "Testing accuracy: 0.978\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Training loss: 0.013036774601911504\n",
      "Training accuracy: 0.9988166666666667\n",
      "Testing loss: 0.07096431074533494\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Training loss: 0.012773270760724941\n",
      "Training accuracy: 0.99885\n",
      "Testing loss: 0.0704099485342195\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Training loss: 0.012454075061281522\n",
      "Training accuracy: 0.9989166666666667\n",
      "Testing loss: 0.07419131074477414\n",
      "Testing accuracy: 0.9772\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Training loss: 0.012222504636086524\n",
      "Training accuracy: 0.999\n",
      "Testing loss: 0.07221401224927206\n",
      "Testing accuracy: 0.978\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Training loss: 0.012050279820462069\n",
      "Training accuracy: 0.9989666666666667\n",
      "Testing loss: 0.07935049038107513\n",
      "Testing accuracy: 0.9755\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Training loss: 0.011791406195114056\n",
      "Training accuracy: 0.9991\n",
      "Testing loss: 0.07064647884308331\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Training loss: 0.011582180770610769\n",
      "Training accuracy: 0.9991\n",
      "Testing loss: 0.07023684330771018\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Training loss: 0.011332092901940148\n",
      "Training accuracy: 0.9990666666666667\n",
      "Testing loss: 0.07235759238350638\n",
      "Testing accuracy: 0.978\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Training loss: 0.011131467388942837\n",
      "Training accuracy: 0.9990666666666667\n",
      "Testing loss: 0.07155882795623082\n",
      "Testing accuracy: 0.9773\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Training loss: 0.010966237336210907\n",
      "Training accuracy: 0.9991333333333333\n",
      "Testing loss: 0.07178008979391069\n",
      "Testing accuracy: 0.978\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Training loss: 0.010809701678839823\n",
      "Training accuracy: 0.9990666666666667\n",
      "Testing loss: 0.0723940679495027\n",
      "Testing accuracy: 0.9776\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Training loss: 0.01059640109160294\n",
      "Training accuracy: 0.9992666666666666\n",
      "Testing loss: 0.07138002270189037\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Training loss: 0.0103256707812349\n",
      "Training accuracy: 0.9993333333333333\n",
      "Testing loss: 0.07148865329629629\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Training loss: 0.010122741307814915\n",
      "Training accuracy: 0.9993666666666666\n",
      "Testing loss: 0.07168842720722032\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Training loss: 0.010001209698741634\n",
      "Training accuracy: 0.9994333333333333\n",
      "Testing loss: 0.07190730833451435\n",
      "Testing accuracy: 0.9775\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Training loss: 0.009809430104618272\n",
      "Training accuracy: 0.9994833333333333\n",
      "Testing loss: 0.07376781409861413\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 98\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0096693051956594\n",
      "Training accuracy: 0.9994\n",
      "Testing loss: 0.07252712496467362\n",
      "Testing accuracy: 0.9786\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Training loss: 0.009507118334559103\n",
      "Training accuracy: 0.9995166666666667\n",
      "Testing loss: 0.07246557981254842\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Training loss: 0.009324274179153144\n",
      "Training accuracy: 0.9995333333333334\n",
      "Testing loss: 0.07222756499329665\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Training loss: 0.00922258551158011\n",
      "Training accuracy: 0.99955\n",
      "Testing loss: 0.07099083842039346\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Training loss: 0.009007129730843007\n",
      "Training accuracy: 0.99955\n",
      "Testing loss: 0.07203882252411906\n",
      "Testing accuracy: 0.9774\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Training loss: 0.00886084193860491\n",
      "Training accuracy: 0.9995333333333334\n",
      "Testing loss: 0.07225495158801461\n",
      "Testing accuracy: 0.978\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Training loss: 0.008737702565826476\n",
      "Training accuracy: 0.9996\n",
      "Testing loss: 0.07235006815946667\n",
      "Testing accuracy: 0.978\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Training loss: 0.008644282445684076\n",
      "Training accuracy: 0.9997166666666667\n",
      "Testing loss: 0.07709813436973768\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Training loss: 0.008445385699346662\n",
      "Training accuracy: 0.9997166666666667\n",
      "Testing loss: 0.07212777430867141\n",
      "Testing accuracy: 0.9785\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Training loss: 0.0082690479779616\n",
      "Training accuracy: 0.99965\n",
      "Testing loss: 0.07176381807958197\n",
      "Testing accuracy: 0.9788\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Training loss: 0.008173636091469476\n",
      "Training accuracy: 0.9997166666666667\n",
      "Testing loss: 0.07278543392753904\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Training loss: 0.00803638241905719\n",
      "Training accuracy: 0.9997333333333334\n",
      "Testing loss: 0.07313741617509799\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Training loss: 0.007950795170664788\n",
      "Training accuracy: 0.99975\n",
      "Testing loss: 0.0733264302464119\n",
      "Testing accuracy: 0.978\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Training loss: 0.007872397280205041\n",
      "Training accuracy: 0.9998166666666667\n",
      "Testing loss: 0.07285633769478043\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Training loss: 0.0077344604703287285\n",
      "Training accuracy: 0.9997333333333334\n",
      "Testing loss: 0.07160591808923336\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Training loss: 0.0075782736752803125\n",
      "Training accuracy: 0.9997833333333334\n",
      "Testing loss: 0.07320058224782063\n",
      "Testing accuracy: 0.978\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Training loss: 0.00749693930807213\n",
      "Training accuracy: 0.9997833333333334\n",
      "Testing loss: 0.07296673651520685\n",
      "Testing accuracy: 0.978\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Training loss: 0.0073966316527997455\n",
      "Training accuracy: 0.9997833333333334\n",
      "Testing loss: 0.0730757277107065\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Training loss: 0.007290417713361481\n",
      "Training accuracy: 0.9998333333333334\n",
      "Testing loss: 0.07259676603757581\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Training loss: 0.007165730042445163\n",
      "Training accuracy: 0.9998166666666667\n",
      "Testing loss: 0.0733410974610063\n",
      "Testing accuracy: 0.9787\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Training loss: 0.007082293008267879\n",
      "Training accuracy: 0.9998166666666667\n",
      "Testing loss: 0.07567472358168975\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Training loss: 0.006979945343049864\n",
      "Training accuracy: 0.99985\n",
      "Testing loss: 0.07382840672389242\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Training loss: 0.006866855443703632\n",
      "Training accuracy: 0.9998833333333333\n",
      "Testing loss: 0.07386578958270965\n",
      "Testing accuracy: 0.978\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Training loss: 0.006783375405830642\n",
      "Training accuracy: 0.99985\n",
      "Testing loss: 0.07218780570699104\n",
      "Testing accuracy: 0.978\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Training loss: 0.006705393606921037\n",
      "Training accuracy: 0.9999\n",
      "Testing loss: 0.07364348435509731\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Training loss: 0.0065957901541764535\n",
      "Training accuracy: 0.9998833333333333\n",
      "Testing loss: 0.07307661404302285\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Training loss: 0.006528339222601305\n",
      "Training accuracy: 0.99985\n",
      "Testing loss: 0.07398242176329468\n",
      "Testing accuracy: 0.9776\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Training loss: 0.00645363445263356\n",
      "Training accuracy: 0.9999\n",
      "Testing loss: 0.07352090154620256\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Training loss: 0.006366409360182782\n",
      "Training accuracy: 0.9999\n",
      "Testing loss: 0.07271178956974036\n",
      "Testing accuracy: 0.9784\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Training loss: 0.006285547265596688\n",
      "Training accuracy: 0.9999166666666667\n",
      "Testing loss: 0.07371755727050695\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Training loss: 0.006159374620571422\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07347528949442207\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Training loss: 0.00612137025819781\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07427615641503577\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Training loss: 0.006043232561958333\n",
      "Training accuracy: 0.9998666666666667\n",
      "Testing loss: 0.07330135926218692\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Training loss: 0.005964965575033178\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07362694229788272\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Training loss: 0.005909095387222866\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.0732034024690533\n",
      "Testing accuracy: 0.978\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Training loss: 0.005816084269434214\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07365913535942839\n",
      "Testing accuracy: 0.9785\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Training loss: 0.005710894801405569\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.0736601230092228\n",
      "Testing accuracy: 0.9789\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Training loss: 0.005704706728210052\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07363033556278534\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Training loss: 0.005594754570334529\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07380383206897706\n",
      "Testing accuracy: 0.9779\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Training loss: 0.005565431064305206\n",
      "Training accuracy: 0.9999166666666667\n",
      "Testing loss: 0.07353169746510833\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Training loss: 0.005468878140983483\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07345674532409283\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Training loss: 0.005410230713772277\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07434358469441912\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Training loss: 0.005356378569453955\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07336329236725986\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Training loss: 0.005290522420344253\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07420023693054303\n",
      "Testing accuracy: 0.9783\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Training loss: 0.005241965782828629\n",
      "Training accuracy: 0.9999333333333333\n",
      "Testing loss: 0.07400989732156348\n",
      "Testing accuracy: 0.9787\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Training loss: 0.005164995361926655\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07378924007647653\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Training loss: 0.005095662221343567\n",
      "Training accuracy: 0.9999666666666667\n",
      "Testing loss: 0.0742827490376973\n",
      "Testing accuracy: 0.9777\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Training loss: 0.005072993428229044\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07409450172032912\n",
      "Testing accuracy: 0.9786\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Training loss: 0.005019008654821664\n",
      "Training accuracy: 0.9999333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 0.07428963483640462\n",
      "Testing accuracy: 0.9781\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Training loss: 0.004943513346556574\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07377906280620154\n",
      "Testing accuracy: 0.9778\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Training loss: 0.004911605197004974\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07399287143541251\n",
      "Testing accuracy: 0.978\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Training loss: 0.004851995894561211\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07437987723402607\n",
      "Testing accuracy: 0.9787\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Training loss: 0.004765998089375595\n",
      "Training accuracy: 0.9999666666666667\n",
      "Testing loss: 0.07489689214619998\n",
      "Testing accuracy: 0.9779\n",
      "Done\n",
      "Total Time Consumed: 2234.873111963272\n"
     ]
    }
   ],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(1) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82bd48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'net_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b72760",
   "metadata": {},
   "source": [
    "### Seed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "047c48bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 0.0047495700763538476\n",
      "Training accuracy: 0.99995\n",
      "Testing loss: 0.07491658408280794\n",
      "Testing accuracy: 0.978\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training loss: 0.004693165396464367\n",
      "Training accuracy: 0.9999666666666667\n",
      "Testing loss: 0.07470454824057698\n",
      "Testing accuracy: 0.9782\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j9/stxf0ygx7r17jmzqd5vp91dm0000gn/T/ipykernel_1315/3381551095.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {i+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss_record\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtest_loss_record\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/j9/stxf0ygx7r17jmzqd5vp91dm0000gn/T/ipykernel_1315/763263482.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mrunning_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Compute prediction and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \"\"\"\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(2) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4cc73a",
   "metadata": {},
   "source": [
    "### Seed 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a45f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(3) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a0a40a",
   "metadata": {},
   "source": [
    "### Seed 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(4) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db57a54d",
   "metadata": {},
   "source": [
    "### Seed 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "torch.manual_seed(5) \n",
    "\n",
    "# Data Loading\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss_record = []\n",
    "train_loss_record =[]\n",
    "for i in range(num_epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loss_record.append(train_loop(train_loader,model,loss,optimizer))\n",
    "    test_loss_record.append(test_loop(test_loader,model,loss))\n",
    "print('Done')\n",
    "print('Total Time Consumed: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce979c",
   "metadata": {},
   "source": [
    "## 3. (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268e5e06",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb5345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.01 \n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "model = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011aa181",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c70c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.2\n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "model = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814851b",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04fb2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.5\n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "model = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16298204",
   "metadata": {},
   "source": [
    "### Momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8529a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.1 \n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "momentum = 0.5\n",
    "model = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum = momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b97d52",
   "metadata": {},
   "source": [
    "### Momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f217bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HyperParameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.1 \n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "momentum = 0.9\n",
    "model = ANN(input_size, num_classes)\n",
    "\n",
    "# Define Loss & Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum = momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667ae5a",
   "metadata": {},
   "source": [
    "# 5. More about Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc5225",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c12fb710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_data = pd.read_csv('../data/train.txt', header = None)\n",
    "test_data = pd.read_csv('../data/test.txt', header = None)\n",
    "val_data = pd.read_csv('../data/val.txt', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "540898a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1569)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aebc8c",
   "metadata": {},
   "source": [
    "<font size=\"3\">From above, we can see that the number of rows is 20000. We know that each sample is composed of a total of 1568 pixels and one label. Therfore, we know that the pixels are scanned out in row-major order. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f71b407d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0.0\n",
       "4       0.0\n",
       "       ... \n",
       "1563    0.0\n",
       "1564    0.0\n",
       "1565    0.0\n",
       "1566    0.0\n",
       "1567    0.0\n",
       "Name: 4, Length: 1568, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[4][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9a31577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleplot(index):\n",
    "    x = train_data.loc[index][:-1]\n",
    "    y = train_data.loc[index][-1:]\n",
    "    x = np.array(x).reshape(28,56)\n",
    "    imshow(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aa12123d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568    12.0\n",
      "Name: 4, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampleplot(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d2698eb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568    15.0\n",
      "Name: 5, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampleplot(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6651123f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568    9.0\n",
      "Name: 6, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampleplot(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae2f6b",
   "metadata": {},
   "source": [
    "<font size=\"3\"> From above, we can see that the sum of two digits equal to the last coordinate of each line. </font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
